# ü¶ô Ollama Linux Installer

Script d'installation automatis√© pour **Ollama** sur Linux avec d√©tection intelligente de l'architecture, support GPU (NVIDIA/AMD) et configuration systemd compl√®te.

## üìñ Qu'est-ce qu'Ollama ?

[Ollama](https://ollama.com) permet d'ex√©cuter des mod√®les de langage (LLM) en local sur votre machine Linux. Ce script automatise enti√®rement son installation et sa configuration.

## ‚ú® Fonctionnalit√©s

### Installation Automatique
- üîç D√©tection automatique de l'architecture (amd64, arm64)
- üì• T√©l√©chargement depuis ollama.com
- üîß Installation dans `/usr/local` ou `/usr`
- üßπ Nettoyage des versions pr√©c√©dentes
- üîó Cr√©ation de liens symboliques automatiques

### Support GPU
- üéÆ **NVIDIA** : Installation automatique des drivers CUDA
- üî¥ **AMD** : Installation automatique ROCm
- ü§ñ **NVIDIA JetPack** : Support Jetson (JetPack 5 et 6)
- ü™ü **WSL2** : D√©tection GPU via nvidia-smi
- ‚ö° D√©tection via `lspci` ou `lshw`

### Configuration Syst√®me
- üë§ Cr√©ation utilisateur syst√®me `ollama`
- ‚öôÔ∏è Configuration **systemd service** automatique
- üéØ Ajout aux groupes `render` et `video` pour GPU
- üöÄ D√©marrage automatique au boot
- üåê API disponible sur `127.0.0.1:11434`

### Distributions Support√©es
- Ubuntu / Debian
- CentOS / RHEL
- Fedora
- Rocky Linux
- Amazon Linux
- Arch Linux
- WSL2 (Windows Subsystem for Linux)

## üöÄ Installation Rapide

### M√©thode Simple (Recommand√©e)

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Installation depuis ce D√©p√¥t

```bash
# Cloner le d√©p√¥t
git clone https://github.com/ledokter/ollama-installer.git
cd ollama-installer

# Rendre ex√©cutable
chmod +x install-ollama.sh

# Installer
sudo ./install-ollama.sh
```

### Installer une Version Sp√©cifique

```bash
export OLLAMA_VERSION="0.1.20"
curl -fsSL https://ollama.com/install.sh | sh
```

## üìã Pr√©requis

### Outils Requis

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install curl pciutils lshw -y

# CentOS/RHEL/Rocky
sudo yum install curl pciutils lshw -y

# Fedora
sudo dnf install curl pciutils lshw -y

# Arch Linux
sudo pacman -S curl pciutils lshw
```

### D√©pendances

- `curl` - t√©l√©chargement
- `awk`, `grep`, `sed` - parsing
- `sudo` - privil√®ges √©lev√©s
- `lspci` ou `lshw` - d√©tection GPU (optionnel)

## üíª Utilisation

### Commandes de Base

```bash
# T√©l√©charger un mod√®le
ollama pull llama3

# Lancer une conversation
ollama run llama3

# Lister les mod√®les
ollama list

# Supprimer un mod√®le
ollama rm llama3
```

### Gestion du Service

```bash
# V√©rifier le statut
sudo systemctl status ollama

# D√©marrer
sudo systemctl start ollama

# Arr√™ter
sudo systemctl stop ollama

# Red√©marrer
sudo systemctl restart ollama

# Voir les logs
sudo journalctl -u ollama -f
```

### API REST

L'API est accessible sur `http://127.0.0.1:11434` :

```bash
# G√©n√©rer du texte
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Pourquoi le ciel est bleu ?"
}'

# Lister les mod√®les
curl http://localhost:11434/api/tags
```

## üéÆ Support GPU

### NVIDIA GPU

Le script installe automatiquement :
- D√©p√¥ts CUDA officiels NVIDIA
- Drivers NVIDIA (cuda-drivers ou nvidia-driver-latest)
- Kernel headers si n√©cessaire
- Modules kernel (nvidia, nvidia_uvm)

**D√©tection** : Vendor ID `10de` (NVIDIA)

**Distributions support√©es** :
- Ubuntu, Debian
- CentOS/RHEL 7, 8, 9
- Rocky Linux
- Fedora (jusqu'√† 39)
- Amazon Linux

### AMD GPU

T√©l√©charge automatiquement la version ROCm optimis√©e.

**D√©tection** : Vendor ID `1002` (AMD)

### NVIDIA JetPack

Support natif pour :
- **JetPack 6** (R36) - Jetson Orin
- **JetPack 5** (R35) - Jetson Xavier/Nano

D√©tection via `/etc/nv_tegra_release`

### Mode CPU Uniquement

Si aucun GPU n'est d√©tect√©, Ollama fonctionne en mode CPU avec un avertissement.

## ü™ü Support WSL2

Le script d√©tecte automatiquement WSL2 et adapte l'installation.

**Pr√©requis WSL2** :
- Systemd activ√©
- GPU passthrough NVIDIA configur√©

**Activer systemd** :

√âditez `/etc/wsl.conf` :
```ini
[boot]
systemd=true
```

Red√©marrez WSL :
```powershell
wsl --shutdown
```

**Note** : WSL1 n'est **pas support√©**.

## üìÇ Architecture d'Installation

### Chemins Cr√©√©s

```
/usr/local/ollama              # Binaire principal
/usr/local/bin/ollama          # Lien symbolique
/usr/share/ollama              # Home utilisateur ollama
/etc/systemd/system/ollama.service  # Service systemd
/etc/modules-load.d/nvidia.conf     # Modules NVIDIA (si GPU)
```

### Utilisateur et Groupes

```bash
# Utilisateur syst√®me
ollama:x:999:999::/usr/share/ollama:/bin/false

# Groupes
- ollama (groupe principal)
- render (acc√®s GPU)
- video (acc√®s GPU)
```

### Service Systemd

Fichier `/etc/systemd/system/ollama.service` :

```ini
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

[Install]
WantedBy=default.target
```

## ‚öôÔ∏è Configuration Avanc√©e

### Variables d'Environnement

```bash
# Version sp√©cifique
export OLLAMA_VERSION="0.1.20"
./install-ollama.sh

# Forcer mode CPU (pas de GPU)
export OLLAMA_SKIP_GPU=1
./install-ollama.sh
```

### Personnaliser le Service

√âditez le service :
```bash
sudo systemctl edit ollama
```

Ajoutez des variables :
```ini
[Service]
# √âcouter sur toutes les interfaces
Environment="OLLAMA_HOST=0.0.0.0:11434"

# Utiliser plusieurs GPUs
Environment="OLLAMA_NUM_GPU=2"

# Augmenter la limite m√©moire
Environment="OLLAMA_MAX_LOADED_MODELS=5"
```

Red√©marrer :
```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

### Changer le Port

```bash
sudo systemctl edit ollama
```

```ini
[Service]
Environment="OLLAMA_HOST=127.0.0.1:11435"
```

```bash
sudo systemctl restart ollama
```

## üîß D√©sinstallation

```bash
# Arr√™ter et d√©sactiver le service
sudo systemctl stop ollama
sudo systemctl disable ollama

# Supprimer les fichiers
sudo rm -rf /usr/local/ollama
sudo rm /usr/local/bin/ollama
sudo rm /etc/systemd/system/ollama.service

# Supprimer l'utilisateur
sudo userdel -r ollama

# Recharger systemd
sudo systemctl daemon-reload
```

## üêõ D√©pannage

### Le script demande sudo

```bash
# Ex√©cuter avec sudo
sudo ./install-ollama.sh

# Ou en tant que root
su -
./install-ollama.sh
```

### Architecture non support√©e

Le script supporte uniquement `x86_64`, `aarch64`, `arm64`.

V√©rifiez votre architecture :
```bash
uname -m
```

### Le service ne d√©marre pas

```bash
# V√©rifier les logs
sudo journalctl -u ollama -n 50 --no-pager

# Tester manuellement
/usr/local/bin/ollama serve

# V√©rifier les permissions
ls -la /usr/local/ollama
```

### GPU non d√©tect√©

```bash
# V√©rifier la d√©tection
lspci | grep -i nvidia
lspci | grep -i amd

# V√©rifier les drivers NVIDIA
nvidia-smi

# R√©installer les tools de d√©tection
sudo apt install pciutils lshw
```

### Erreur de t√©l√©chargement

```bash
# Tester la connectivit√©
curl -I https://ollama.com/download/ollama-linux-amd64.tgz

# Via proxy
export http_proxy=http://proxy:8080
export https_proxy=http://proxy:8080
./install-ollama.sh
```

### Port 11434 d√©j√† utilis√©

```bash
# Trouver le processus
sudo lsof -i :11434

# Changer le port d'Ollama
sudo systemctl edit ollama
# Ajouter : Environment="OLLAMA_HOST=127.0.0.1:11435"

sudo systemctl restart ollama
```

### Permissions GPU

```bash
# V√©rifier l'appartenance aux groupes
groups $(whoami)

# Ajouter manuellement aux groupes
sudo usermod -aG render,video $(whoami)

# D√©connexion/reconnexion requise
```

## üìä Exemple d'Installation Compl√®te

```bash
$ sudo ./install-ollama.sh
>>> Installing ollama to /usr/local
>>> Downloading Linux amd64 bundle
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
>>> Making ollama accessible in the PATH in /usr/local/bin
>>> Creating ollama user...
>>> Adding ollama user to render group...
>>> Adding ollama user to video group...
>>> Adding current user to ollama group...
>>> Creating ollama systemd service...
>>> Enabling and starting ollama service...
>>> NVIDIA GPU detected.
>>> Installing NVIDIA repository...
>>> Installing CUDA driver...
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100%
>>> NVIDIA GPU ready.
>>> The Ollama API is now available at 127.0.0.1:11434.
>>> Install complete. Run "ollama" from the command line.

$ ollama --version
ollama version 0.1.20

$ ollama pull llama3
pulling manifest
pulling ff82381e2bea... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 4.7 GB
pulling 43070e2d4e53... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 11 KB
success

$ ollama run llama3
>>> Bonjour, comment √ßa va ?
Bonjour ! Je vais bien, merci ! Comment puis-je vous aider aujourd'hui ?
```

## üîí S√©curit√©

### Par D√©faut

- ‚úÖ API √©coute sur `127.0.0.1` uniquement (localhost)
- ‚úÖ Utilisateur d√©di√© sans shell (`/bin/false`)
- ‚úÖ Pas d'authentification n√©cessaire en local
- ‚úÖ Permissions restrictives sur les fichiers

### Exposition Publique (‚ö†Ô∏è Attention)

**Pour exposer l'API sur le r√©seau** :

```bash
sudo systemctl edit ollama
```

```ini
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
```

```bash
sudo systemctl restart ollama
```

**‚ö†Ô∏è DANGER** : Aucune authentification par d√©faut !

**Recommandation** : Utilisez un reverse proxy avec auth :

```nginx
# nginx
location /ollama/ {
    auth_basic "Ollama API";
    auth_basic_user_file /etc/nginx/.htpasswd;
    proxy_pass http://127.0.0.1:11434/;
}
```

## üìö Ressources

### Documentation Officielle

- [Ollama Documentation](https://github.com/ollama/ollama/tree/main/docs)
- [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Model Library](https://ollama.com/library)
- [Modelfile Guide](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)

### Guides GPU

- [NVIDIA CUDA Installation](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/)
- [AMD ROCm Installation](https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html)
- [WSL2 GPU Support](https://learn.microsoft.com/en-us/windows/wsl/tutorials/gpu-compute)

### Communaut√©

- [Ollama GitHub](https://github.com/ollama/ollama)
- [Discussions](https://github.com/ollama/ollama/discussions)
- [Discord](https://discord.gg/ollama)

## ü§ù Contribution

Les contributions sont bienvenues !

1. Fork ce d√©p√¥t
2. Cr√©ez une branche : `git checkout -b feature/amelioration`
3. Testez sur plusieurs distributions
4. Committez : `git commit -m "Description"`
5. Push : `git push origin feature/amelioration`
6. Ouvrez une Pull Request

### Guidelines

- Testez sur Ubuntu, Debian, CentOS minimum
- Conservez la compatibilit√© POSIX shell
- Ajoutez des messages de status clairs
- Documentez les nouvelles fonctionnalit√©s

## üìù Changelog

### v1.0 (2026-02-03)
- üéâ Documentation compl√®te pour GitHub
- ‚ú® Support NVIDIA CUDA automatique
- ‚ú® Support AMD ROCm
- ‚ú® Support NVIDIA JetPack (5 et 6)
- ‚ú® D√©tection WSL2
- ‚ú® Configuration systemd compl√®te
- ‚ú® Installation multi-distributions

## ‚öñÔ∏è Licence

MIT License

```
Copyright (c) 2026 Ollama Team

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## üôè Cr√©dits

- **Ollama Team** - Script d'installation original
- **ledokter** - Documentation et packaging GitHub
- Communaut√© NVIDIA - Documentation CUDA
- Communaut√© AMD - Documentation ROCm

## üì¨ Contact

**Documentation par** : [ledokter](https://github.com/ledokter)

**Script original** : [Ollama](https://ollama.com)

---

‚≠ê **Si ce guide vous aide, donnez une √©toile au projet !**
```
